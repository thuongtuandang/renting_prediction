{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to create addition features for text. Tokenizers using pre-trained model like BERT usually have high dimension (at least 768). And if we concat these vectors to our dataset, it will cost a lot because it requires a lot of resources to train with large amount of data. Using autogluon for multi-modal model is also a problem for me, because my current laptop does not have GPU, and I think autogluon also process text data via tokenizing, and later use neural network to train the model. \n",
    "\n",
    "So an optimal solution for me is to use dimension reduction with PCA. Tokenizer methods can capture well similarities among texts (by self-attention) and high-level semantic similarities are more likely to be preserved after PCA because they contribute more to the variance across embeddings. For example, if two texts are about very similar topics, this major semantic feature is likely to be captured even in a reduced dimensionality space. \n",
    "\n",
    "The initial principal components (which explain the most variance) are likely to capture more general, high-level features of the text data. If the primary use case involves understanding or clustering texts based on broad topics or sentiments, PCA-reduced embeddings may still be effective.\n",
    "\n",
    "Because of limited resources, I will perform the method on only 2% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample size: (161, 25)\n",
      "Validation sample size: (46, 25)\n",
      "Test sample size: (46, 25)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv('../data/train_data.csv')\n",
    "df_val = pd.read_csv('../data/val_data.csv')\n",
    "df_test = pd.read_csv('../data/test_data.csv')\n",
    "\n",
    "# Create samples with 1% of each DataFrame\n",
    "df_train_sample = df_train.sample(frac=0.001, random_state=1)\n",
    "df_val_sample = df_val.sample(frac=0.001, random_state=1)\n",
    "df_test_sample = df_test.sample(frac=0.001, random_state=1)\n",
    "\n",
    "# Optionally, you can check the sizes of your samples\n",
    "print(f\"Train sample size: {df_train_sample.shape}\")\n",
    "print(f\"Validation sample size: {df_val_sample.shape}\")\n",
    "print(f\"Test sample size: {df_test_sample.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sample.to_csv('../data/train_data_sample.csv', index=False)\n",
    "df_val_sample.to_csv('../data/val_data_sample.csv', index=False)\n",
    "df_test_sample.to_csv('../data/test_data_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khanhdam/.local/share/virtualenvs/renting-JHnSpTKC/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-german-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vect(input_text):\n",
    "    # Max token is 512\n",
    "    # We truncate longer input text\n",
    "    encoded_input = tokenizer(input_text, return_tensors='tf', max_length=512, truncation=True)\n",
    "    model_output = bert_model(encoded_input)\n",
    "    embeddings = model_output.last_hidden_state[:,0,:]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def concat_vectors_with_pca(df, text_column, n_components=10):\n",
    "    # Generate embeddings\n",
    "    vectors = np.zeros((len(df), 768))\n",
    "    for i, text in enumerate(df[text_column]):\n",
    "        vectors[i, :] = text2vect(text)\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_vectors = pca.fit_transform(vectors)\n",
    "    \n",
    "    # Concatenate reduced embeddings with the original DataFrame\n",
    "    df_copy = df.copy()\n",
    "    vector_columns = [f'{text_column}_vec_{i}' for i in range(reduced_vectors.shape[1])]\n",
    "    df_copy_with_vectors = pd.concat([\n",
    "        df_copy.reset_index(drop=True),\n",
    "        pd.DataFrame(reduced_vectors, columns=vector_columns),\n",
    "    ], axis=1)\n",
    "    \n",
    "    return df_copy_with_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = ['description', 'facilities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sample = concat_vectors_with_pca(df_train_sample,'description')\n",
    "df_train_sample = concat_vectors_with_pca(df_train_sample, 'facilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161, 45)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(161, 43)\n"
     ]
    }
   ],
   "source": [
    "df_train_sample.drop(columns = text_columns, axis=1, inplace=True)\n",
    "print(df_train_sample.shape)\n",
    "df_train_sample.to_csv('../data/vect_train_data_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 43)\n"
     ]
    }
   ],
   "source": [
    "df_val_sample = concat_vectors_with_pca(df_val_sample,'description')\n",
    "df_val_sample = concat_vectors_with_pca(df_val_sample, 'facilities')\n",
    "df_val_sample.drop(columns = text_columns, axis=1, inplace=True)\n",
    "print(df_val_sample.shape)\n",
    "df_val_sample.to_csv('../data/vect_val_data_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 43)\n"
     ]
    }
   ],
   "source": [
    "df_test_sample = concat_vectors_with_pca(df_test_sample,'description')\n",
    "df_test_sample = concat_vectors_with_pca(df_test_sample, 'facilities')\n",
    "df_test_sample.drop(columns = text_columns, axis=1, inplace=True)\n",
    "print(df_test_sample.shape)\n",
    "df_test_sample.to_csv('../data/vect_test_data_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
